from __future__ import annotations

from abc import ABC, abstractmethod
from datetime import datetime
from pathlib import Path
from typing import Any, ClassVar, Dict, List, Optional, Type, TypeVar, Union
import json
import uuid

import jsonschema

from .quantumcircuit import QasmEmitterOptions
from .benchmarkschema import SCHEMA_VERSION, BENCHMARK_JSON_SCHEMA


B = TypeVar("B", bound="Benchmark")


class Benchmark(ABC):
    """Abstract base class for all quantum device benchmarks.

    This class provides shared configuration and functionality for concrete
    benchmark implementations.

    Responsibilities:
      * Store common configuration such as:
          - ``number_of_qubits``
          - ``sample_size``
          - ``emitter_options`` (QasmEmitterOptions)
      * Provide a common API:
          - `create_benchmark()` – generate benchmark samples.
          - `to_json_dict()` – produce a JSON-serialisable payload.
          - `save_json()` – write the benchmark to disk.
          - `load_json()` / `from_json_dict()` – reconstruct from JSON.
      * Provide helpers for attaching and evaluating experimental results.

    Subclasses must implement:
      * `_create_single_sample()`
      * `evaluate_benchmark()`

    Subclasses may override:
      * `_compute_number_of_measurements()`

    Attributes:
      BENCHMARK_NAME: Logical name of the benchmark, written to JSON and used
        when reloading. Subclasses are expected to override this constant.
      DEFAULT_WORKDIR: Default directory where benchmarks are stored if no
        explicit path is given.
    """

    BENCHMARK_NAME: ClassVar[str] = "Benchmark"
    DEFAULT_WORKDIR: ClassVar[Path] = Path(".benchmarks")

    def __init__(
        self,
        number_of_qubits: int,
        sample_size: int = 10,
        *,
        emitter_options: Optional[QasmEmitterOptions] = None,
        format: Optional[str] = None,
        target_sdk: Optional[str] = None,
        shots: Optional[int] = 1,
        print_defaults: bool = False,
        workdir: Optional[Union[str, Path]] = None,
        benchmark_id: Optional[str] = None,
        auto_save: bool = True,
    ) -> None:
        """Initialize a `Benchmark` instance.

        Args:
          number_of_qubits: Number of qubits used in each circuit.
          sample_size: Number of samples (base circuits / setups) to generate
            for this benchmark.
          emitter_options: Pre-constructed `QasmEmitterOptions`. If provided,
            this takes precedence over `format` and `target_sdk`.
          format: Circuit representation format (for example, `"qasm2"`). This
            is ignored if `emitter_options` is provided.
          target_sdk: Name of the target SDK (for example, `"qiskit"`, `"cirq"`).
            This is ignored if `emitter_options` is provided.
          shots: Number of shots per circuit when running experimental
            executions. This value is written to the JSON payload and may be
            used by subclasses when interpreting results.
          print_defaults: If True, prints information when default values are
            used for `format` and/or `target_sdk`.
          workdir: Optional working directory that controls where JSON files
            will be written by `save_json()`. If omitted, `DEFAULT_WORKDIR` is
            used.
          benchmark_id: Optional externally assigned benchmark identifier. If
            omitted, a new identifier is generated by `_generate_benchmark_id()`.
          auto_save: If True, operations that modify the benchmark (for example,
            `create_benchmark()` or `add_experimental_results()`) will
            automatically persist the updated JSON representation to disk.

        """
        # Internal state / in-memory dataset
        self.samples: Optional[List[Dict[str, Any]]] = None
        self.benchmark_metadata: Dict[str, Any] = {}
        self.path: Optional[Path] = None
        self.experimental_results: Optional[Dict[str, Any]] = None

        # Auto-save behavior
        self.auto_save: bool = bool(auto_save)

        # Input normalization
        self.number_of_qubits = int(number_of_qubits)
        self.sample_size = int(sample_size)
        self.shots = int(shots) if shots is not None else None

        # Working directory: instance-level override, or class default
        self.workdir = self.DEFAULT_WORKDIR if workdir is None else Path(workdir)

        # Benchmark identifier
        self.benchmark_id = benchmark_id or self._generate_benchmark_id()

        # Emitter options
        if emitter_options is not None:
            self.emitter_options = emitter_options
            self.format = emitter_options.format
            self.target_sdk = emitter_options.target_sdk
        else:
            eff_format = (format or "qasm2").lower()
            eff_target = (target_sdk or "default").lower()

            if print_defaults:
                if format is None:
                    print("Default format for circuit representation is 'qasm2'.")
                if target_sdk is None:
                    print("Default target_sdk 'default' is used.")

            self.emitter_options = QasmEmitterOptions(
                format=eff_format,
                target_sdk=eff_target,
            )
            self.format = eff_format
            self.target_sdk = eff_target

    def __repr__(self) -> str:
        """Return a concise, developer-oriented representation.

        Returns:
          str: A debug-friendly string containing the main configuration
          fields of the benchmark instance.
        """
        return (
            f"{self.__class__.__name__}(\n"
            f"  benchmark_id={self.benchmark_id!r},\n"
            f"  number_of_qubits={self.number_of_qubits},\n"
            f"  sample_size={self.sample_size},\n"
            f"  format={self.format!r},\n"
            f"  target_sdk={self.target_sdk!r},\n"
            f"  workdir={self.workdir!r},\n"
            f")"
        )

    def create_benchmark(
        self,
        sample_size: Optional[int] = None,
        auto_save: Optional[bool] = None,
        save_to: Optional[Union[str, Path]] = None,
    ) -> List[Dict[str, Any]]:
        """Create and store benchmark samples in memory.

        This method calls `_create_single_sample()` once per sample and stores
        the resulting list on `samples`. Optionally, it will also serialize the
        benchmark to JSON on disk immediately after generation.

        Args:
          sample_size: Optional override for the number of samples to generate.
            If provided, this value replaces the existing `sample_size`.
          auto_save: Optional override for the instance-level `auto_save`
            setting. If not None, the instance's `auto_save` flag is updated
            before any saving occurs.
          save_to: Optional explicit file path or directory to write the JSON
            to. If omitted and `auto_save` is True, the default filename
            (`{benchmark_id}.json` under `workdir`) is used. If `save_to`
            refers to a directory, the benchmark is saved inside that
            directory using the default filename.

        Returns:
          list[dict[str, Any]]: A list of sample dictionaries. Each sample is
          expected to conform to the JSON schema for `"sample"` objects, for
          example::

            {
                "sample_id": int,
                "sample_metadata": {...},
                "circuits": [
                    {
                        "circuit_id": str,
                        "observable": str | null,
                        "qasm": str,
                        "metadata": {...},
                    },
                    ...
                ],
            }
        """
        if sample_size is not None:
            self.sample_size = int(sample_size)

        samples: List[Dict[str, Any]] = []
        for idx in range(self.sample_size):
            sample = self._create_single_sample(idx)
            samples.append(sample)

        self.samples = samples

        if auto_save is not None:
            self.auto_save = bool(auto_save)

        if self.auto_save:
            if save_to is not None:
                saved_path = self.save_json(filepath=save_to)
            else:
                saved_path = self.save_json()

            print(f"[Benchmark] Saved to: {saved_path}")

        return samples

    def _generate_benchmark_id(self) -> str:
        """Generate a unique, human-readable benchmark identifier.

        The benchmark identifier is designed to be:

          * Stored as `"benchmark_id"` in the JSON payload.
          * Used as the basis of the default filename (``{benchmark_id}.json``).
          * A convenient key for cataloging, indexing, or experiment tracking.

        The identifier encodes:
          * The benchmark name (`BENCHMARK_NAME` or class name).
          * The number of qubits.
          * The sample size.
          * A timestamp.
          * A short random UUID fragment.

        Format:
          `{benchmark_name}_nq{qubits}_s{samples}_{timestamp}_{uuid8}`

        Example:
          `"clifford_nq5_s10_20250203T153012_ab12cd34"`

        Returns:
          str: A newly generated benchmark ID.
        """
        benchmark_name = getattr(self, "BENCHMARK_NAME", self.__class__.__name__)
        timestamp = datetime.now().strftime("%Y%m%dT%H%M%S")
        uid = uuid.uuid4().hex[:8]

        return (
            f"{benchmark_name}"
            f"_nq{self.number_of_qubits}"
            f"_s{self.sample_size}"
            f"_{timestamp}_{uid}"
        )

    def _compute_number_of_measurements(self) -> int:
        """Compute the number of distinct measurement schemes.

        By default, this returns 1 and assumes that each sample corresponds to
        a single measurement scheme per circuit.

        Subclasses may override this method to indicate that they generate more
        than one measurement scheme per base unitary.

        Returns:
          int: The number of distinct measurement schemes used by this
          benchmark.
        """
        return 1

    # !!! The following method must be implemented by subclasses

    @abstractmethod
    def _create_single_sample(self, sample_id: int) -> Dict[str, Any]:
        """Create a single benchmark sample.

        Subclasses must implement this method to build a single sample
        dictionary.

        Args:
          sample_id: Integer identifier for the sample (0-based index assigned
            by `create_benchmark()`).

        Returns:
          dict[str, Any]: A dictionary representing a single sample, with the
          structure expected by the benchmark JSON schema. A typical shape is::

            {
                "sample_id": int,
                "sample_metadata": {...},
                "circuits": [
                    {
                        "circuit_id": str,
                        "observable": str | None,
                        "qasm": str,
                        "metadata": {...},
                    },
                    ...
                ],
            }
        """
        raise NotImplementedError

    # Filename / path helpers methods

    def _default_filename(self) -> str:
        """Return the default filename for this benchmark JSON file.

        The filename is based on the current `benchmark_id` and has the form
        `{benchmark_id}.json`.

        Returns:
          str: The default filename for the benchmark JSON file.
        """
        return f"{self.benchmark_id}.json"

    def _default_filepath(self) -> Path:
        """Compute the default full path for this benchmark JSON file.

        The default path is given by `self.workdir / _default_filename()`.

        Returns:
          pathlib.Path: A path object pointing to the default file location.
        """
        return self.workdir / self._default_filename()

    def _jsonify_value(self, v: Any) -> Any:
        """Convert a value to a JSON-serialisable representation.

        This helper is primarily intended for metadata values. It transparently
        converts objects that implement `tolist()` (such as NumPy arrays) to
        their list representation.

        Args:
          v: Value to convert.

        Returns:
          Any: A JSON-serialisable value (for example, built-in Python types).
        """
        if hasattr(v, "tolist"):
            return v.tolist()
        return v

    def _jsonify_meta(self, meta: Dict[str, Any]) -> Dict[str, Any]:
        """Convert all values in a metadata dictionary to JSON-friendly types.

        Args:
          meta: Metadata dictionary with potentially non-JSON types.

        Returns:
          dict[str, Any]: A new dictionary with the same keys, where all values
          are JSON-serialisable.
        """
        return {k: self._jsonify_value(v) for k, v in meta.items()}

    def to_json_dict(
        self,
        *,
        global_metadata: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """Build a JSON-serialisable dictionary representing the benchmark.

        The returned dictionary is designed to conform to
        `BENCHMARK_JSON_SCHEMA`.

        Args:
          global_metadata: Optional metadata to include at the top level of the
            JSON payload. If omitted, `benchmark_metadata` is used.

        Returns:
          dict[str, Any]: A JSON-serialisable dictionary ready to be passed to
          `json.dump()`.

        Raises:
          ValueError: If `samples` is None (that is, the benchmark data has not
            been generated yet).
        """
        if self.samples is None:
            raise ValueError(
                "The benchmark dataset was not generated. "
                "Call create_benchmark() first."
            )

        global_metadata = global_metadata or self.benchmark_metadata

        samples_list: List[Dict[str, Any]] = []
        for sample in self.samples:
            sample_copy = dict(sample)

            # JSONify sample_metadata if present
            sm = sample_copy.get("sample_metadata", {})
            sample_copy["sample_metadata"] = self._jsonify_meta(sm)

            # JSONify metadata for each circuit
            circuits = sample_copy.get("circuits", [])
            new_circuits = []
            for circuit in circuits:
                circ_copy = dict(circuit)
                md = circ_copy.get("metadata", {})
                circ_copy["metadata"] = self._jsonify_meta(md)
                new_circuits.append(circ_copy)

            sample_copy["circuits"] = new_circuits
            samples_list.append(sample_copy)

        benchmark_to_save: Dict[str, Any] = {
            "schema_version": SCHEMA_VERSION,
            "benchmark_name": getattr(self, "BENCHMARK_NAME", self.__class__.__name__),
            "benchmark_id": self.benchmark_id,
            "number_of_qubits": self.number_of_qubits,
            "sample_size": self.sample_size,
            "format": getattr(self, "format", None),
            "target_sdk": getattr(self, "target_sdk", None),
            "shots": getattr(self, "shots", None),
            "global_metadata": self._jsonify_meta(global_metadata),
            "samples": samples_list,
        }

        # Only include experimental_results if we actually have them
        if self.experimental_results is not None:
            benchmark_to_save["experimental_results"] = self.experimental_results

        return benchmark_to_save

    def save_json(
        self,
        filepath: Optional[Union[str, Path]] = None,
        *,
        global_metadata: Optional[Dict[str, Any]] = None,
        indent: int = 2,
    ) -> Path:
        """Serialise the benchmark to a JSON file on disk.

        The target path is resolved according to the following rules:

        * If `filepath` is None:
            The benchmark is stored under `workdir` with an auto-generated
            filename (see `_default_filename()`).

        * If `filepath` is an existing directory:
            The benchmark is stored inside that directory with an
            auto-generated filename, and `workdir` is updated to that
            directory.

        * If `filepath` refers to a non-existing path with no suffix
          (for example, `"results"`) and the path does not yet exist:
            It is treated as a directory; the directory is created as needed
            and the benchmark is stored inside it with an auto-generated
            filename. `workdir` is updated to that directory.

        * If `filepath` is a bare filename (no directory component):
            The file is stored under `workdir` with that name.

        * If `filepath` contains a directory component (absolute or relative),
          for example, `"results/foo.json"` or `"/tmp/foo.json"`:
            The file is stored exactly at that path and `workdir` is updated to
            its parent directory. Any missing parent directories are created.

        Args:
          filepath: Optional target path. See rules above.
          global_metadata: Optional global metadata to include in the JSON
            payload. If omitted, `benchmark_metadata` is used.
          indent: Indentation level passed to `json.dump()`.

        Returns:
          pathlib.Path: The final path where the JSON file was saved.
        """
        payload = self.to_json_dict(global_metadata=global_metadata)

        if filepath is None:
            # No filepath → use workdir + auto name
            path = self._default_filepath()
        else:
            p = Path(filepath)

            # Case 1: Existing directory
            if p.exists() and p.is_dir():
                path = p / self._default_filename()
                self.workdir = p

            # Case 2: Non-existing path without suffix → treat as directory
            elif not p.exists() and p.suffix == "":
                path = p / self._default_filename()
                self.workdir = p

            # Case 3: Bare filename (no directory component)
            elif not p.is_absolute() and p.parent == Path("."):
                path = self.workdir / p.name
                # workdir remains unchanged

            # Case 4: Path with directory component (absolute or relative)
            else:
                path = p
                self.workdir = p.parent

        # Ensure directory exists
        path.parent.mkdir(parents=True, exist_ok=True)

        with open(path, "w", encoding="utf-8") as f:
            json.dump(payload, f, indent=indent)

        self.path = path

        return path

    @classmethod
    def from_json_dict(
        cls: Type[B],
        data: Dict[str, Any],
        *,
        validate_schema: bool = True,
        strict_benchmark_name: bool = True,
    ) -> B:
        """Construct an instance of `cls` from a JSON dictionary.

        This is the in-memory counterpart of `to_json_dict()`. It assumes that
        `data` conforms to `BENCHMARK_JSON_SCHEMA` and expects (at minimum) the
        keys `"schema_version"`, `"number_of_qubits"` and `"sample_size"`.

        Args:
          data: Parsed JSON object (for example, returned by `json.load()`).
          validate_schema: If True, validates `data` against
            `BENCHMARK_JSON_SCHEMA` using `jsonschema`.
          strict_benchmark_name: If True, checks that the `"benchmark_name"`
            field (if present) matches `BENCHMARK_NAME` of `cls`. If the field
            is present and does not match, a `ValueError` is raised.

        Returns:
          B: An instance of `cls` with `samples`, `benchmark_metadata` and
          configuration loaded from `data`.

        Raises:
          ValueError: If the schema version is missing or incompatible, if
            benchmark names do not match (when `strict_benchmark_name` is
            True), or if the JSON fails validation against
            `BENCHMARK_JSON_SCHEMA`.
        """
        #  Basic sanity checks
        if "schema_version" not in data:
            raise ValueError("Missing 'schema_version' in benchmark JSON.")
        if data["schema_version"] != SCHEMA_VERSION:
            raise ValueError(
                f"Incompatible schema_version: {data['schema_version']} "
                f"(expected {SCHEMA_VERSION})."
            )

        if validate_schema:
            try:
                jsonschema.validate(instance=data, schema=BENCHMARK_JSON_SCHEMA)
            except jsonschema.ValidationError as exc:
                raise ValueError(
                    f"Benchmark JSON failed schema validation: {exc.message}"
                ) from exc

        benchmark_name = data.get("benchmark_name")
        class_name = getattr(cls, "BENCHMARK_NAME", cls.__name__)
        if strict_benchmark_name and benchmark_name is not None:
            if benchmark_name != class_name:
                raise ValueError(
                    f"JSON benchmark_name={benchmark_name!r} does not match "
                    f"class BENCHMARK_NAME={class_name!r}."
                )

        #  Extract constructor arguments
        number_of_qubits = data["number_of_qubits"]
        benchmark_id = data.get("benchmark_id")
        sample_size = data["sample_size"]
        format_ = data.get("format")
        target_sdk = data.get("target_sdk")
        shots = data.get("shots")

        #  Make an instance using the normal constructor.

        inst = cls(
            number_of_qubits=number_of_qubits,
            sample_size=sample_size,
            format=format_,
            target_sdk=target_sdk,
            shots=shots,
            benchmark_id=benchmark_id,
        )

        #  Attach metadata and samples
        inst.benchmark_metadata = data.get("global_metadata", {})
        inst.samples = data.get("samples", [])
        inst.experimental_results = data.get("experimental_results")

        return inst

    @classmethod
    def load_json(
        cls: Type[B],
        filepath: Union[str, Path],
        *,
        validate_schema: bool = True,
        strict_benchmark_name: bool = True,
    ) -> B:
        """Load benchmark JSON from a file and construct an instance.

        This is a convenience wrapper around `from_json_dict()`. After loading,
        the instance's `workdir` is set to the directory containing the JSON
        file.

        Args:
          filepath: Path to the JSON file to load.
          validate_schema: If True, validates the parsed JSON object against
            `BENCHMARK_JSON_SCHEMA`.
          strict_benchmark_name: If True, checks that the `"benchmark_name"`
            field (if present) matches `BENCHMARK_NAME` of `cls`. See
            `from_json_dict()` for details.

        Returns:
          B: An instance of `cls` constructed from the JSON file.

        Raises:
          ValueError: If the file content fails schema or benchmark-name checks
            in `from_json_dict()`.
          OSError: If the file cannot be opened or read.
          json.JSONDecodeError: If the file does not contain valid JSON.
        """
        path = Path(filepath)

        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)

        inst = cls.from_json_dict(
            data,
            validate_schema=validate_schema,
            strict_benchmark_name=strict_benchmark_name,
        )

        # New workdir: folder containing the JSON file
        inst.workdir = path.parent

        return inst

    def add_experimental_results(
        self,
        counts_data: Union[
            Dict[str, Dict[str, int]],  # circuit_id -> counts dict
            List[Dict[str, int]],  # list aligned with circuit order
        ],
        *,
        experiment_id: Optional[str] = None,
        platform: str = "Unknown",
        experiment_metadata: Optional[Dict[str, Any]] = None,
        auto_save: Optional[bool] = None,
        save_to: Optional[Union[str, Path]] = None,
    ) -> None:
        """Attach experimental results (counts) to this benchmark.

        This populates `experimental_results` in a shape that matches the
        `"experimental_results"` block of `BENCHMARK_JSON_SCHEMA`.

        The counts data can be provided in two forms:

        1. Dict keyed by `circuit_id` (recommended)::

             counts_data = {
                 "0_stab_0": {"000": 512, "111": 488},
                 "0_destab_0": {"000": 500, "111": 500},
                 ...
             }

        2. List in the same order as the circuits appear in `self.samples`::

             counts_data = [
                 {"000": 512, "111": 488},   # for first circuit
                 {"000": 500, "111": 500},   # for second circuit
                 ...
             ]

        Args:
          counts_data: Either a mapping `circuit_id -> {state: count}`, or a
            list of `{state: count}` dictionaries ordered by circuit
            traversal.
          experiment_id: Identifier for this particular experimental run. If
            None, `benchmark_id` is used.
          platform: Name of the platform / backend (for example, `"ibm"`,
            `"ionq"`, `"simulator"`).
          experiment_metadata: Optional additional metadata about the run.
          auto_save: Optional override for the instance-level `auto_save`
            setting before saving the updated benchmark JSON.
          save_to: Optional file path or directory where the updated JSON
            file should be stored. See `save_json()` for resolution rules.

        Raises:
          ValueError: If `samples` have not been generated or if the number of
            provided count entries does not match the number of circuits.
          TypeError: If counts for a circuit are not provided as a dict mapping
            bitstring to count, or if counts cannot be converted to integers.
        """
        if self.samples is None:
            raise ValueError(
                "Cannot attach experimental results: samples are not generated. "
                "Call create_benchmark() or load_json() first."
            )

        experiment_id = experiment_id or self.benchmark_id

        # Build mapping circuit_id -> counts dict

        if isinstance(counts_data, dict):
            # Assume already keyed by circuit_id
            counts_by_circuit: Dict[str, Dict[str, int]] = counts_data
        else:
            # Treat as a list aligned with circuit order in self.samples
            flat_circuit_ids: List[str] = []
            for sample in self.samples:
                for circuit in sample.get("circuits", []):
                    flat_circuit_ids.append(circuit["circuit_id"])

            if len(flat_circuit_ids) != len(counts_data):
                raise ValueError(
                    f"Length mismatch: got {len(counts_data)} result entries "
                    f"but benchmark has {len(flat_circuit_ids)} circuits."
                )

            counts_by_circuit = {
                cid: cd for cid, cd in zip(flat_circuit_ids, counts_data)
            }

        if auto_save is not None:
            self.auto_save = bool(auto_save)

        # Normalise and validate counts

        results_payload: Dict[str, Dict[str, Dict[str, int]]] = {}

        for circuit_id, counts in counts_by_circuit.items():
            if not isinstance(counts, dict):
                raise TypeError(
                    f"Counts for circuit_id={circuit_id!r} must be a dict "
                    f"mapping bitstring -> count, got {type(counts).__name__}."
                )

            norm_counts: Dict[str, int] = {}
            for state, count in counts.items():
                # Convert state to string and count to int
                state_str = str(state)
                try:
                    count_int = int(count)
                except (TypeError, ValueError) as exc:
                    raise TypeError(
                        f"Non-integer count for state {state_str!r} "
                        f"in circuit_id={circuit_id!r}: {count!r}"
                    ) from exc

                if count_int < 0:
                    raise ValueError(
                        f"Negative count for state {state_str!r} "
                        f"in circuit_id={circuit_id!r}: {count_int}"
                    )

                norm_counts[state_str] = count_int

            results_payload[circuit_id] = {"counts": norm_counts}

        self.experimental_results = {
            "experiment_id": experiment_id,
            "platform": platform,
            "experiment_metadata": experiment_metadata or {},
            "results": results_payload,
        }

        #  Auto-save

        if self.auto_save:
            # If user specified a path, use that
            if save_to is not None:
                saved_path = self.save_json(filepath=save_to)
            # Else, if we already have a path (benchmark was saved before), reuse it
            elif self.path is not None:
                saved_path = self.save_json(filepath=self.path)
            # Otherwise, use default path (workdir + auto filename)
            else:
                saved_path = self.save_json()

            print(f"[Benchmark] Saved to: {saved_path}")

    def get_all_circuit_ids(self) -> List[str]:
        """Return a flat list of all circuit IDs in traversal order.

        The order matches the internal sample/circuit ordering, which is the
        same order used when counts are provided as a flat list.

        Returns:
          list[str]: All circuit IDs in the order they appear in `samples`.

        Raises:
          ValueError: If `samples` have not been generated or loaded.
        """
        if self.samples is None:
            raise ValueError(
                "Cannot retrieve circuit IDs; samples have not been generated. "
                "Call create_benchmark() or load_json() first."
            )

        circuit_ids: List[str] = []
        for sample in self.samples:
            for circuit in sample.get("circuits", []):
                circuit_ids.append(circuit["circuit_id"])
        return circuit_ids

    def get_all_circuits(self) -> List[str]:
        """Return a flat list of QASM strings in canonical order.

        The order matches the benchmark's internal sample/circuit ordering::

            samples[0].circuits[0].qasm
            samples[0].circuits[1].qasm
            ...
            samples[1].circuits[0].qasm
            ...

        Returns:
          list[str]: One QASM string per circuit.

        Raises:
          ValueError: If samples have not been generated or loaded.
        """
        if self.samples is None:
            raise ValueError(
                "Cannot retrieve QASM circuits; samples have not been generated. "
                "Call create_benchmark() or load_json() first."
            )

        qasm_list: List[str] = []
        for sample in self.samples:
            for circuit in sample.get("circuits", []):
                qasm_list.append(circuit["qasm"])

        return qasm_list

    # Methods for benchmark evaluations

    def expected_value(
        self,
        counts: dict[str, int],
        pauli: str,
        *,
        little_endian: bool = False,
    ) -> float:
        """Compute the expectation value of a Pauli operator from counts.

        The input is a classical bitstring distribution and a tensor-product
        Pauli label. The bitstrings are assumed to correspond to Z-basis
        measurements on each qubit.

        Args:
          counts: Mapping from bitstring (for example, `"010"`) to counts.
          pauli: Pauli label, for example, `"XYZ"`, `"+XZI"`, or `"-XYI"`.
            The first character may be `'+'` or `'-'`; if omitted, `'+'` is
            assumed.
          little_endian: If True, interpret bitstrings Qiskit-style:
            the rightmost bit corresponds to qubit 0 (least significant).

        Returns:
          float: Expectation value in the interval ``[-1, 1]``.

        Raises:
          ValueError: If `counts` is empty, the Pauli length does not match the
            bitstring length, or the total shot count is zero.
        """
        if not counts:
            raise ValueError("counts dict is empty")

        # Extract sign and Pauli operators
        sign = -1 if pauli.startswith("-") else 1
        ops = pauli[1:] if pauli[0] in "+-" else pauli

        bitlen = len(next(iter(counts)))
        if len(ops) != bitlen:
            raise ValueError(
                f"Pauli operator has length {len(ops)}, "
                f"but bitstrings have length {bitlen}."
            )

        # Reverse if using little-endian (Qiskit convention)
        if little_endian:
            ops = ops[::-1]

        total_shots = sum(counts.values())
        if total_shots == 0:
            raise ValueError("Total shot count is zero")

        acc = 0
        for bitstring, count in counts.items():
            if little_endian:
                bitstring = bitstring[::-1]

            # Compute eigenvalue: product of ±1 from each qubit
            parity = sum((op != "I" and bit == "1") for op, bit in zip(ops, bitstring))
            eigen = -1 if parity % 2 else 1
            acc += eigen * count

        return sign * acc / total_shots

    # !!! The following method must be implemented by subclasses !!!
    @abstractmethod
    def evaluate_benchmark(self) -> Any:
        """Evaluate this benchmark using attached experimental results.

        Subclasses must implement this method.

        Typical responsibilities:
          * Read `samples` and `experimental_results`.
          * Compute benchmark-specific quantities (for example,
            expectation values, fidelities, success probabilities, volumes).
          * Optionally store derived quantities back into
            `experimental_results` or additional attributes.

        Returns:
          Any: A benchmark-specific evaluation result, for example, a dict of
          metrics. The exact shape is defined by the subclass.
        """
        raise NotImplementedError
